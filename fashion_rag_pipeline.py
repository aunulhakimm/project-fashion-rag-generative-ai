# -*- coding: utf-8 -*-
"""fashion_rag_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mfpt_GKuDUp32BUImS34juQVv8OY82oF?usp=sharing
"""

# ========================================
# INSTALL DEPENDENCIES (Colab only)
# ========================================
# Uncomment this cell if you run in Google Colab
# If you run locally (VS Code / Jupyter), install via pip in terminal:
# pip install transformers torch torchvision faiss-cpu pillow matplotlib numpy pandas kagglehub sentencepiece

# !pip install -q transformers torch torchvision faiss-cpu pillow matplotlib numpy pandas kagglehub sentencepiece

!pip install transformers torch torchvision faiss-cpu pillow matplotlib numpy pandas kagglehub sentencepiece

# ========================================
# IMPORT LIBRARIES & DEVICE SETUP
# ========================================

import os
import torch
import faiss
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm

from transformers import CLIPProcessor, CLIPModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import kagglehub

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ========================================
# STEP 1: DOWNLOAD & LOAD KAGGLE DATASET
# ========================================

print("\n[1/8] Downloading Fashion Product Dataset from Kaggle...")

# Download dataset
path = kagglehub.dataset_download("nirmalsankalana/fashion-product-text-images-dataset")
print("Path to dataset files:", path)

# Cek struktur folder
print("\nDataset structure:")
for root, dirs, files in os.walk(path):
    level = root.replace(path, '').count(os.sep)
    indent = ' ' * 2 * level
    print(f'{indent}{os.path.basename(root)}/')
    subindent = ' ' * 2 * (level + 1)
    for file in files[:5]:  # Show first 5 files
        print(f'{subindent}{file}')
    if len(files) > 5:
        print(f'{subindent}... and {len(files)-5} more files')
    if level > 2:  # Limit depth
        break

# Load metadata CSV (otomatis pilih CSV pertama)
csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
print(f"\nFound CSV files: {csv_files}")

if csv_files:
    metadata_path = os.path.join(path, csv_files[0])
    df = pd.read_csv(metadata_path)
    print(f"\nDataset loaded: {len(df)} products")
    print("\nDataset columns:")
    print(df.columns.tolist())
    print("\nFirst 3 rows:")
    display(df.head(3))
else:
    print("No CSV file found. Creating dummy metadata...")
    # Cari semua gambar
    image_files = []
    for root, dirs, files in os.walk(path):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                image_files.append(os.path.join(root, file))

    df = pd.DataFrame({
        'image_path': image_files[:1000],  # Limit untuk demo
        'product_name': [f'Product_{i}' for i in range(len(image_files[:1000]))],
        'category': ['Fashion' for _ in range(len(image_files[:1000]))]
    })

print("\nDataframe ready.")

# ========================================
# STEP 2: PREPARE IMAGE PATHS & VALID DATA
# ========================================

print("\n[2/8] Preparing image paths...")

# Identifikasi kolom image path
image_col = None
for col in df.columns:
    if 'image' in col.lower() or 'path' in col.lower() or 'filename' in col.lower():
        image_col = col
        break

if image_col is None:
    print("Error: Cannot find image column in dataset")
    print("Available columns:", df.columns.tolist())
    # Manual assignment (adjust based on your CSV)
    image_col = df.columns[0]  # Assume first column

print(f"Using image column: {image_col}")

def get_full_image_path(img_name: str) -> str:
    """
    Mencari path lengkap file gambar di dalam folder dataset.
    - Jika img_name sudah path lengkap dan exists -> return langsung
    - Jika hanya nama file, cari di semua subfolder dari `path`
    """
    # Jika sudah full path
    if os.path.exists(img_name):
        return img_name

    # Cari di subdirectory
    for root, dirs, files in os.walk(path):
        if img_name in files:
            return os.path.join(root, img_name)
        # Try basename
        if os.path.basename(img_name) in files:
            return os.path.join(root, os.path.basename(img_name))

    return None

# Validasi image paths
valid_data = []
print("Validating image files...")
for idx, row in tqdm(df.iterrows(), total=len(df)):
    img_path = get_full_image_path(str(row[image_col]))
    if img_path and os.path.exists(img_path):
        valid_data.append({
            'image_path': img_path,
            'data': row.to_dict()
        })
    # Limit untuk demo, hapus atau perbesar jika ingin full dataset
    if len(valid_data) >= 20000:
        break

print(f"\nValid images found: {len(valid_data)}")

if len(valid_data) == 0:
    raise RuntimeError("ERROR: No valid images found! Check dataset structure and image paths.")
else:
    print(f"Sample image path: {valid_data[0]['image_path']}")

# ========================================
# STEP 3: LOAD CLIP MODEL (EMBEDDING)
# ========================================

print("\n[3/8] Loading CLIP model...")

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

clip_model.eval()
print("CLIP model loaded successfully!")

# ========================================
# STEP 4: EXTRACT IMAGE EMBEDDINGS
# ========================================

print("\n[4/8] Extracting image embeddings...")

def extract_image_embedding(image_path: str):
    """Extract embedding dari file gambar menggunakan CLIP."""
    try:
        image = Image.open(image_path).convert('RGB')
        inputs = clip_processor(images=image, return_tensors="pt").to(device)

        with torch.no_grad():
            image_features = clip_model.get_image_features(**inputs)

        # Normalize embedding (cosine similarity)
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)

        return image_features.cpu().numpy().flatten()
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
        return None

embeddings = []
metadata = []

print(f"Processing {len(valid_data)} images...")
for item in tqdm(valid_data):
    emb = extract_image_embedding(item['image_path'])
    if emb is not None:
        embeddings.append(emb)
        metadata.append(item)

embeddings = np.array(embeddings).astype('float32')

print(f"""\nEmbeddings extracted: {len(embeddings)}
Embeddings shape: {embeddings.shape}""")

# ========================================
# STEP 5: BUILD FAISS INDEX & RETRIEVAL FUNCTIONS
# ========================================

print("\n[5/8] Building FAISS index...")

if embeddings.ndim != 2 or embeddings.shape[0] == 0:
    raise RuntimeError("Embeddings array is empty or invalid.")

# Dimensi embedding CLIP
d = embeddings.shape[1]

# Buat index (IndexFlatIP untuk cosine similarity dengan vektor ter-normalisasi)
index = faiss.IndexFlatIP(d)

# Normalize embeddings untuk cosine similarity
faiss.normalize_L2(embeddings)

# Tambah embeddings ke index
index.add(embeddings)

print(f"FAISS index built with {index.ntotal} vectors")

def search_by_image(query_image_path: str, top_k: int = 5):
    """
    Cari gambar mirip berdasarkan file gambar.
    Return: (distances, indices)
    """
    query_embedding = extract_image_embedding(query_image_path)
    if query_embedding is None:
        return None, None

    query_embedding = query_embedding.reshape(1, -1).astype('float32')
    faiss.normalize_L2(query_embedding)
    distances, indices = index.search(query_embedding, top_k)
    return distances[0], indices[0]

def search_by_text(query_text: str, top_k: int = 5):
    """
    Cari gambar berdasarkan teks query menggunakan encoder teks CLIP.
    Return: (distances, indices)
    """
    inputs = clip_processor(text=[query_text], return_tensors="pt", padding=True).to(device)

    with torch.no_grad():
        text_features = clip_model.get_text_features(**inputs)

    text_features = text_features / text_features.norm(dim=-1, keepdim=True)
    text_embedding = text_features.cpu().numpy().astype('float32')

    faiss.normalize_L2(text_embedding)
    distances, indices = index.search(text_embedding, top_k)
    return distances[0], indices[0]

print("Retrieval functions ready: search_by_image, search_by_text")

# ========================================
# STEP 6: VISUALIZATION (OPTIONAL, UNTUK CEK HASIL RETRIEVAL)
# ========================================

def visualize_results(query, results_indices, distances, query_type: str = "image"):
    """
    Visualisasi hasil retrieval (query image atau text query).
    query_type: "image" atau "text".
    """
    n_results = len(results_indices)

    if query_type == "image":
        fig, axes = plt.subplots(1, n_results + 1, figsize=(18, 3))

        # Query image
        query_img = Image.open(query).convert('RGB')
        axes[0].imshow(query_img)
        axes[0].set_title("Query Image", fontweight='bold', fontsize=10)
        axes[0].axis('off')

        # Hasil retrieval
        for i, (idx, dist) in enumerate(zip(results_indices, distances)):
            img_path = metadata[idx]['image_path']
            img = Image.open(img_path).convert('RGB')
            axes[i+1].imshow(img)

            info = metadata[idx]['data']
            title = f"Score: {dist:.3f}\n"
            for key in ['product_name', 'productDisplayName', 'name', 'title', 'category']:
                if key in info:
                    title += f"{str(info[key])[:20]}"
                    break

            axes[i+1].set_title(title, fontsize=8)
            axes[i+1].axis('off')
    else:
        fig, axes = plt.subplots(1, n_results, figsize=(15, 3))
        fig.suptitle(f'Text Query: "{query}"', fontsize=14, fontweight='bold')

        for i, (idx, dist) in enumerate(zip(results_indices, distances)):
            img_path = metadata[idx]['image_path']
            img = Image.open(img_path).convert('RGB')

            ax = axes if n_results == 1 else axes[i]
            ax.imshow(img)

            info = metadata[idx]['data']
            title = f"Score: {dist:.3f}\n"
            for key in ['product_name', 'productDisplayName', 'name', 'title', 'category']:
                if key in info:
                    title += f"{str(info[key])[:20]}"
                    break

            ax.set_title(title, fontsize=8)
            ax.axis('off')

    plt.tight_layout()
    plt.show()

print("Visualization helper ready: visualize_results")

# ========================================
# STEP 7: GENERATIVE COMPONENT (RAG TEXT)
# ========================================

print("\n[6/8] Loading generative model (Flan-T5) for RAG...")

#  loda model generatif flan T5, ini  ini buat deksripsi fas
gen_model_name = "google/flan-t5-small"  # bisa diganti base kalau resource cukup
gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)
gen_model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name).to(device)
gen_model.eval()

print("Flan-T5 model loaded successfully!")

def build_context_from_indices(indices, max_len_per_item: int = 80):
    """
    Bangun teks konteks dari metadata hasil retrieval.
    Digunakan sebagai augmented context untuk generative model.
    """
    context_lines = []
    for idx in indices:
        info = metadata[idx]['data']
        pieces = []

        for key in ['product_name', 'productDisplayName', 'name', 'title']:
            if key in info and pd.notna(info[key]):
                pieces.append(str(info[key]))
                break

        for key in ['description', 'product_description', 'detail_desc']:
            if key in info and pd.notna(info[key]):
                pieces.append(str(info[key]))
                break

        if 'category' in info and pd.notna(info['category']):
            pieces.append(f"(Kategori: {info['category']})")

        if pieces:
            text_item = " ".join(pieces)
            text_item = text_item[:max_len_per_item]
            context_lines.append(f"- {text_item}")

    if not context_lines:
        return "Tidak ada deskripsi produk yang tersedia."

    return "\n".join(context_lines)

def generate_rag_description(context_text: str, max_new_tokens: int = 120):
    """
    Menghasilkan deskripsi generatif berbasis teks konteks hasil retrieval.
    -> Inilah bagian 'G' dari RAG (Augmented Generation).
    """
    prompt = (
        "Kamu adalah sistem rekomendasi produk fashion.\n"
        "Berikut adalah daftar produk yang relevan dengan query pengguna:\n\n"
        f"{context_text}\n\n"
        "Buatlah deskripsi singkat dalam bahasa Indonesia yang merangkum gaya, warna, "
        "dan kesan umum dari produk-produk di atas. Tulis seperti rekomendasi e-commerce."
    )

    inputs = gen_tokenizer(prompt, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        outputs = gen_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            min_length=30,
            num_beams=4,
            early_stopping=True,
            no_repeat_ngram_size=3,
            repetition_penalty=1.5,
            length_penalty=1.0,
            temperature=0.8,
            top_k=50,
            top_p=0.95
        )
    summary = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return summary

def rag_from_text_query(query_text: str, top_k: int = 5):
    """
    Pipeline lengkap: text query -> retrieval -> bangun context -> generatif.
    Return: distances, indices, context_text, rag_text
    """
    distances, indices = search_by_text(query_text, top_k=top_k)
    context = build_context_from_indices(indices)
    rag_text = generate_rag_description(context)
    return distances, indices, context, rag_text

def rag_from_image_query(query_image_path: str, top_k: int = 5):
    """
    Pipeline lengkap: image query -> retrieval -> bangun context -> generatif.
    Return: distances, indices, context_text, rag_text
    """
    distances, indices = search_by_image(query_image_path, top_k=top_k)
    context = build_context_from_indices(indices)
    rag_text = generate_rag_description(context)
    return distances, indices, context, rag_text

print("RAG functions ready: build_context_from_indices, generate_rag_description, rag_from_text_query, rag_from_image_query")

# ========================================
# STEP 8: DEMO RETRIEVAL + RAG (OPTIONAL)
# ========================================

print("\n[7/8] Running demo retrieval + RAG...")

# Demo 1: Image-to-Image + RAG
if len(metadata) > 0:
    print("\n" + "=" * 60)
    print("DEMO 1: Image-to-Image Retrieval + RAG")
    print("=" * 60)

    query_idx = min(10, len(metadata) - 1)
    query_image_path = metadata[query_idx]['image_path']

    print(f"Query image: {os.path.basename(query_image_path)}")
    print(f"Query info: {metadata[query_idx]['data']}")

    distances, indices, context, rag_text = rag_from_image_query(query_image_path, top_k=5)

    if distances is not None:
        print("\nTop 5 similar images (info singkat):")
        for i, (idx, dist) in enumerate(zip(indices, distances)):
            info = metadata[idx]['data']
            name = info.get('product_name', info.get('productDisplayName', ''))
            print(f"{i+1}. Similarity: {dist:.4f} | {name}")

        visualize_results(query_image_path, indices, distances, query_type="image")

        print("\n--- CONTEXT UNTUK RAG ---")
        print(context)
        print("\n--- DESKRIPSI GENERATIF (RAG) ---")
        print(rag_text)

# Demo 2: Text-to-Image + RAG
print("\n" + "=" * 60)
print("DEMO 2: Text-to-Image Retrieval + RAG")
print("=" * 60)

text_queries = [
    "blue jeans",
    "red dress",
    "black shoes",
    "white shirt",
    "casual jacket"
]

for text_query in text_queries[:2]:
    print(f"\nQuery: '{text_query}'")
    distances, indices, context, rag_text = rag_from_text_query(text_query, top_k=5)

    print("Top 5 matches (similarity only):")
    for i, (idx, dist) in enumerate(zip(indices, distances)):
        info = metadata[idx]['data']
        name = info.get('product_name', info.get('productDisplayName', ''))
        print(f"{i+1}. Similarity: {dist:.4f} | {name}")

    visualize_results(text_query, indices, distances, query_type="text")

    print("\n--- CONTEXT UNTUK RAG ---")
    print(context)
    print("\n--- DESKRIPSI GENERATIF (RAG) ---")
    print(rag_text)

print("\n" + "=" * 60)
print("âœ… Retrieval + RAG demo completed.")
print("=" * 60)

# ========================================
# STEP 9: SAVE INDEX & METADATA
# ========================================

print("\n[8/8] Saving FAISS index and metadata...")

faiss.write_index(index, "fashion_product.index")

import pickle
with open("fashion_metadata.pkl", "wb") as f:
    pickle.dump(metadata, f)

print("âœ… Index saved as: fashion_product.index")
print("âœ… Metadata saved as: fashion_metadata.pkl")

print("\nðŸ“– To load in future sessions:")
print("""import faiss
import pickle

index = faiss.read_index("fashion_product.index")
with open("fashion_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

print(f"Index loaded with {index.ntotal} vectors")
""")